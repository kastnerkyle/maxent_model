{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique(arr, dic=None):\n",
    "    if (dic is None):\n",
    "        dic = {}\n",
    "    for el in arr:\n",
    "        if isinstance(el, list):\n",
    "            unique(el, dic)\n",
    "        else:\n",
    "            if (el not in dic):\n",
    "                dic[el] = 1\n",
    "            else:\n",
    "                dic[el] += 1\n",
    "    return np.array(dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification will be according to the following formula:\n",
    "$$p(c\\mid d,\\lambda)=\\frac\n",
    "{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(c,d\\right )}\n",
    "{\\sum_{\\tilde{c}\\in C}{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(\\tilde{c},d\\right )}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, weights, feature_patterns, n_classes):\n",
    "    # initial reduction\n",
    "    probas = np.ones(n_classes) * np.log(1.0 / n_classes)\n",
    "\n",
    "    # we consider the conditional probabilities\n",
    "    for xi in x:\n",
    "        for i in xrange(len(feature_patterns[xi])):\n",
    "            probas[feature_patterns[xi][i]] += weights[xi][i]\n",
    "\n",
    "    # further smooth the outputs through a softmax\n",
    "    probas = np.exp(probas / n_classes)\n",
    "    return probas / np.sum(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem will be solved by maximizing the likelihood function\n",
    "$$\\log p(C|D,\\lambda)\n",
    "=\\sum_{(c,d)\\in(C,D)}\\log p(c|d,\\lambda)\n",
    "=\\sum_{(c,d)\\in(C,D)}\\log\\frac\n",
    "{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(c,d\\right )}\n",
    "{\\sum_{\\tilde{c}\\in C}{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(\\tilde{c},d\\right )}}$$\n",
    "\n",
    "Accordingly, the gradient we will have with respect to lambda\n",
    "$$\\frac{\\partial\\log p(C|D,\\lambda)}{\\partial\\lambda_i}=\n",
    "\\sum_{(c,d)\\in(C,D)}{f_i(c,d)}-\n",
    "\\sum_{d\\in D}{\\sum_{c\\in C}{p(c|d,\\lambda)f_i(c,d)}}$$\n",
    "\n",
    "finally, the update:\n",
    "$$w^{k+1} = w^{k} + \\eta_k\\frac{\\partial}{\\partial w_i}(L(j,w) - \\frac{C}{N}|w_i|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# L(j,w)\n",
    "def apply_l1_penalty(i, j, u, weights, q):\n",
    "    z = weights[i][j]\n",
    "    if weights[i][j] > 0:\n",
    "        weights[i][j] =  max(0.0, weights[i][j] - (u + q[i][j]))\n",
    "    elif weights[i][j] < 0:\n",
    "        weights[i][j] =  max(0.0, weights[i][j] + (u - q[i][j]))\n",
    "    q[i][j] = weights[i][j] - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(X, y, f_count, c_count, batch_size=64, nb_epoch=10, alpha=0.85, max_iter=100, tol=0.00001, l1param=0.1, random_state=None, verbose=1):\n",
    "    '''\n",
    "        X - Context\n",
    "        y - the goal\n",
    "        f_count - the number of unique features in the context\n",
    "        c_count - the number of unique targets upon the required classes\n",
    "        batch_size - sub-sample size for training\n",
    "        alpha - speed training\n",
    "        max_iter - max number of iterations\n",
    "        tol - displacement gradient threshold\n",
    "        l1param - setting l1 regularization\n",
    "        random_state - to reproduce the experiment\n",
    "        verbose - debug\n",
    "    '''\n",
    "    n_samples = len(X)\n",
    "    if batch_size is None:\n",
    "        batch_size = n_samples\n",
    "    if batch_size > n_samples:\n",
    "        batch_size = n_samples\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "        \n",
    "    # # Determine how much we have unique tokens\n",
    "    # Features = unique (X)\n",
    "    # F_count = features.shape [ 0 ]\n",
    "    # # Determine how much we have unique classes\n",
    "    # Classes = unique (y)\n",
    "    # C_count = classes.shape [ 0 ]\n",
    "\n",
    "    # Matrix indicators (conventional signs)\n",
    "    feature_patterns = [[] for _ in range(f_count)]\n",
    "    f_pattern_set = {}\n",
    "    # MAtrix indicator weights\n",
    "    weights = [[] for _ in range(f_count)]\n",
    "    q =  [[] for _ in range(f_count)]\n",
    "    # indicators initialization\n",
    "    for i in range(n_samples):\n",
    "        for xi in X[i]:\n",
    "            if xi not in f_pattern_set:\n",
    "                f_pattern_set[xi] = set()\n",
    "            if y[i] not in f_pattern_set[xi]:\n",
    "                f_pattern_set[xi].add(y[i])\n",
    "                weights[xi].append(0.0)\n",
    "                q[xi].append(0.0)\n",
    "                feature_patterns[xi].append(y[i])\n",
    "    prev_logl = 0.\n",
    "    iter_num = 0\n",
    "    all_iter = 0\n",
    "    u = 0.0\n",
    "    for epoch in range(nb_epoch):\n",
    "        if verbose:\n",
    "            print 'Start epoch #%d\\t' % epoch,\n",
    "        # SGD\n",
    "        # bounded above by max_iter\n",
    "        loss = 0.\n",
    "        for iter_num in range(max_iter):\n",
    "            if verbose and (iter_num % (max_iter / 20) == 0):\n",
    "                print '.',\n",
    "            logl = 0.\n",
    "            ncorrect = 0\n",
    "\n",
    "            r = range(n_samples)\n",
    "            r = random.sample(r, batch_size)\n",
    "            iter_sample = 0\n",
    "            for i in r:\n",
    "                iter_sample += 1\n",
    "\n",
    "\n",
    "                all_iter += 1\n",
    "                eta = alpha ** (all_iter / n_samples)\n",
    "                # Predict the probability\n",
    "                probas = predict(X[i], weights, feature_patterns, c_count)\n",
    "\n",
    "                # If we correctly predicted, it only needs verbose\n",
    "                if np.argmax(probas) == y[i]:\n",
    "                    ncorrect += 1\n",
    "                # We believe \"credibility\"\n",
    "                logl += np.log(probas[y[i]])\n",
    "\n",
    "                u += eta * l1param;\n",
    "                # update the weights\n",
    "                for j in range(len(X[i])):\n",
    "                    conditional_y = feature_patterns[X[i][j]]\n",
    "                    for y_i in xrange(len(conditional_y)):\n",
    "                        # expectation\n",
    "                        expected_ent = 1.0 if conditional_y[y_i] == y[i] else 0.0\n",
    "                        # reality\n",
    "                        max_ent = probas[conditional_y[y_i]]\n",
    "                        weights[X[i][j]][y_i] -= 1.0 * (max_ent - expected_ent) * eta\n",
    "                        apply_l1_penalty(X[i][j],y_i,u,weights,q)\n",
    "            loss += (logl - prev_logl)\n",
    "            prev_logl = logl\n",
    "        if verbose:\n",
    "            print '\\tLoss: %.8f' % (loss/max_iter)\n",
    "    return weights, feature_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch #0\t. . . . . . . . . . . . . . . . . . . . \tLoss: -0.02109188\n",
      "[ 0.94393463  0.05606537]\n"
     ]
    }
   ],
   "source": [
    "# Little test\n",
    "X = [[0, 1],\n",
    "     [2, 1],\n",
    "     [2, 3],\n",
    "     [2, 1],\n",
    "     [0, 1],\n",
    "     [2, 1, 4],\n",
    "     [2, 3, 4],\n",
    "     [2, 1, 5],\n",
    "     [0, 3, 5],\n",
    "     [0, 1, 5]]\n",
    "\n",
    "y = [0, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "# Determine how much we have unique tokens\n",
    "features = unique(X)\n",
    "f_count = features.shape[0]\n",
    "# Determine how much we have unique classes\n",
    "classes = unique(y)\n",
    "c_count = classes.shape[0]\n",
    "weights, patterns = fit(X, y,f_count,c_count, random_state=241,l1param=0.00001,nb_epoch=1)\n",
    "# print weights\n",
    "# print patterns\n",
    "\n",
    "pred = predict([0, 1], weights, patterns,c_count)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_regex = re.compile('\\d')\n",
    "punc_regex = re.compile('[\\%\\(\\)\\-\\/\\:\\;\\<\\>\\«\\»\\,]')\n",
    "delim_regex = re.compile('([\\.])\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_tokenize(foldername):\n",
    "    '''\n",
    "    method for reading text from a file folder\n",
    "    here used quite simple tokenization\n",
    "    '''\n",
    "\n",
    "    word_counts = {}\n",
    "    tokenized_text = []\n",
    "    for path, subdirs, files in os.walk('data'):\n",
    "        for name in files:\n",
    "            filename = os.path.join(path, name)\n",
    "            with io.open(filename, 'r', encoding='utf-8') as data_file:\n",
    "                for line in data_file:\n",
    "                    if len(line) < 50:\n",
    "                        continue\n",
    "                    text = digits_regex.sub(u'0', line.lower())\n",
    "                    text = punc_regex.sub(u'', text)\n",
    "                    text = delim_regex.sub(r' \\1 ', text)\n",
    "                    for word in text.split():\n",
    "                        if not word:\n",
    "                            continue\n",
    "                        if word not in word_counts:\n",
    "                            word_counts[word] = 1\n",
    "                        else:\n",
    "                            word_counts[word] += 1\n",
    "                        tokenized_text.append(word)\n",
    "    word2index = {}\n",
    "    index2word = []\n",
    "    i = 0\n",
    "    filtered_text = []\n",
    "    for word in tokenized_text:\n",
    "        if word_counts[word] > 2:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = i\n",
    "                index2word.append(word)\n",
    "                i += 1\n",
    "            filtered_text.append(word)\n",
    "\n",
    "\n",
    "    return filtered_text, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train(tokenized_text, word2index,context_len = 4):\n",
    "    '''\n",
    "    Method for generating training data\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, y_word in enumerate(tokenized_text):\n",
    "        x = []\n",
    "        for j in range(i - context_len, i):\n",
    "            if (j >= 0):\n",
    "                x_word = tokenized_text[j]\n",
    "                x.append(word2index[x_word])\n",
    "        if (len(x) > 0):\n",
    "            X.append(x)\n",
    "            y.append(word2index[y_word])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text, word2index, index2word = read_and_tokenize('data')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words: 45825\n",
      "all unique words: 3872\n"
     ]
    }
   ],
   "source": [
    "unique_words = len(index2word)\n",
    "print 'all words:', len(tokenized_text)\n",
    "print 'all unique words:', unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_len = 4\n",
    "X,y = generate_train(tokenized_text, word2index,context_len=context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch #0\t. . . . . . . . . . . . . . . . . . . . \tLoss: -5.28181613\n",
      "Start epoch #1\t. . . . . . . . . . . . . . . . . . . . \tLoss: 0.01197383\n"
     ]
    }
   ],
   "source": [
    "weights, patterns = fit(X, y,unique_words,unique_words,random_state=241,verbose=1,batch_size=64, nb_epoch=2, l1param=0.0001,tol=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENRATED TEXT:\n",
      "до на сказал словам сокращение сегодняшний сети по городе реформы в 0000 он декабря совершил за россии правительство в чиновников\n"
     ]
    }
   ],
   "source": [
    "test = [word2index[word] for word in [u'путин']]\n",
    "last_index = index = test[-1]\n",
    "print 'GENRATED TEXT:'\n",
    "for i in range(20):\n",
    "    pred = predict(test, weights, patterns,unique_words)\n",
    "    indicies = pred.argsort()[::-1][:20]\n",
    "    for index in indicies:\n",
    "        if index in test:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    last_index = int(index)\n",
    "    print index2word[index],\n",
    "    test.append(index)\n",
    "    if len(test) > context_len:\n",
    "        del test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENRATED TEXT:\n",
    "According to the said reduction in the present network for city reform in December 0000, he performed for the Russian government officials\n",
    "\n",
    "Ideas to improve\n",
    "The first thing that comes to mind - is to increase the number of training sample\n",
    "used as a context, not words and characters with opredelnie window (context_len) of 40 or more\n",
    "Stemming lematizatsiyu use or for dictionary \"features\", and then combined with the preceding paragraph (while it is not imagine how)\n",
    "model works a bit slow, and more text is very slow. so you can try to look for the optimum parameters of training. You can be rewritten as a solution in C / C ++ or Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "* Tsuruoka Y., Tsujii J., Ananiadou S. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty //Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1. – Association for Computational Linguistics, 2009. – С. 477-485.\n",
    "* Smith N. A., Eisner J. Contrastive estimation: Training log-linear models on unlabeled data //Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. – Association for Computational Linguistics, 2005. – С. 354-362.\n",
    "* Smith N. A. Log-Linear Models // Revised version of thesis research proposal, 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
